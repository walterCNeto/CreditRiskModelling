{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "loan_data_preprocessed_backup = pd.read_csv('loan_data_2007_2014_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_preprocessed = loan_data_preprocessed_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_preprocessed.columns.values\n",
    "# Displays all column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan_data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_preprocessed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults = loan_data_preprocessed[loan_data_preprocessed['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n",
    "# Here we take only the accounts that were charged-off (written-off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loan_data_defaults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None\n",
    "# Sets the pandas dataframe options to display all columns/ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan_data_defaults['mths_since_last_delinq'].fillna(0, inplace = True)\n",
    "# We fill the missing values with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loan_data_defaults['mths_since_last_delinq'].fillna(loan_data_defaults['mths_since_last_delinq'].max() + 12, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n",
    "# We fill the missing values with zeroes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan_data_defaults['recovery_rate'] = loan_data_defaults['recoveries'] / loan_data_defaults['funded_amnt']\n",
    "# We calculate the dependent variable for the LGD model: recovery rate.\n",
    "# It is the ratio of recoveries and funded amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['recovery_rate'].describe()\n",
    "# Shows some descriptive statisics for the values of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] > 1, 1, loan_data_defaults['recovery_rate'])\n",
    "loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] < 0, 0, loan_data_defaults['recovery_rate'])\n",
    "# We set recovery rates that are greater than 1 to 1 and recovery rates that are less than 0 to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['recovery_rate'].describe()\n",
    "# Shows some descriptive statisics for the values of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n",
    "# We calculate the dependent variable for the EAD model: credit conversion factor.\n",
    "# It is the ratio of the difference of the amount used at the moment of default to the total funded amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['CCF'].describe()\n",
    "# Shows some descriptive statisics for the values of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults.to_csv('loan_data_defaults.csv')\n",
    "# We save the data to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loan_data_defaults['recovery_rate'], bins = 100)\n",
    "# We plot a histogram of a variable with 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loan_data_defaults['recovery_rate'], bins = 50)\n",
    "# We plot a histogram of a variable with 50 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loan_data_defaults['CCF'], bins = 100)\n",
    "# We plot a histogram of a variable with 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['recovery_rate_0_1'] = np.where(loan_data_defaults['recovery_rate'] == 0, 0, 1)\n",
    "# We create a new variable which is 0 if recovery rate is 0 and 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_defaults['recovery_rate_0_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGD model stage 1 datasets: recovery rate 0 or greater than 0.\n",
    "lgd_inputs_stage_1_train, lgd_inputs_stage_1_test, lgd_targets_stage_1_train, lgd_targets_stage_1_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['recovery_rate_0_1'], test_size = 0.2, random_state = 42)\n",
    "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
    "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = ['grade:A',\n",
    "'grade:B',\n",
    "'grade:C',\n",
    "'grade:D',\n",
    "'grade:E',\n",
    "'grade:F',\n",
    "'grade:G',\n",
    "'home_ownership:MORTGAGE',\n",
    "'home_ownership:NONE',\n",
    "'home_ownership:OTHER',\n",
    "'home_ownership:OWN',\n",
    "'home_ownership:RENT',\n",
    "'verification_status:Not Verified',\n",
    "'verification_status:Source Verified',\n",
    "'verification_status:Verified',\n",
    "'purpose:car',\n",
    "'purpose:credit_card',\n",
    "'purpose:debt_consolidation',\n",
    "'purpose:educational',\n",
    "'purpose:home_improvement',\n",
    "'purpose:house',\n",
    "'purpose:major_purchase',\n",
    "'purpose:medical',\n",
    "'purpose:moving',\n",
    "'purpose:other',\n",
    "'purpose:renewable_energy',\n",
    "'purpose:small_business',\n",
    "'purpose:vacation',\n",
    "'purpose:wedding',\n",
    "'initial_list_status:f',\n",
    "'initial_list_status:w',\n",
    "'term_int',\n",
    "'emp_length_int',\n",
    "'mths_since_issue_d',\n",
    "'mths_since_earliest_cr_line',\n",
    "'funded_amnt',\n",
    "'int_rate',\n",
    "'installment',\n",
    "'annual_inc',\n",
    "'dti',\n",
    "'delinq_2yrs',\n",
    "'inq_last_6mths',\n",
    "'mths_since_last_delinq',\n",
    "'mths_since_last_record',\n",
    "'open_acc',\n",
    "'pub_rec',\n",
    "'total_acc',\n",
    "'acc_now_delinq',\n",
    "'total_rev_hi_lim']\n",
    "# List of all independent variables for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_reference_cat = ['grade:G',\n",
    "'home_ownership:RENT',\n",
    "'verification_status:Verified',\n",
    "'purpose:credit_card',\n",
    "'initial_list_status:f']\n",
    "# List of the dummy variable reference categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_inputs_stage_1_train = lgd_inputs_stage_1_train[features_all]\n",
    "# Here we keep only the variables we need for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_inputs_stage_1_train = lgd_inputs_stage_1_train.drop(features_reference_cat, axis = 1)\n",
    "# Here we remove the dummy variable reference categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_inputs_stage_1_train.isnull().sum()\n",
    "# Check for missing values. We check whether the value of each row for each column is missing or not,\n",
    "# then sum accross columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P values for sklearn logistic regression.\n",
    "\n",
    "# Class to display p-values for logistic regression in sklearn.\n",
    "\n",
    "from sklearn import linear_model\n",
    "import scipy.stats as stat\n",
    "\n",
    "class LogisticRegression_with_p_values:\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):#,**kwargs):\n",
    "        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model.fit(X,y)\n",
    "        \n",
    "        #### Get p-values for the fitted model ####\n",
    "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
    "        denom = np.tile(denom,(X.shape[1],1)).T\n",
    "        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n",
    "        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
    "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n",
    "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n",
    "        \n",
    "        self.coef_ = self.model.coef_\n",
    "        self.intercept_ = self.model.intercept_\n",
    "        #self.z_scores = z_scores\n",
    "        self.p_values = p_values\n",
    "        #self.sigma_estimates = sigma_estimates\n",
    "        #self.F_ij = F_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lgd_st_1 = LogisticRegression_with_p_values()\n",
    "# We create an instance of an object from the 'LogisticRegression' class.\n",
    "reg_lgd_st_1.fit(lgd_inputs_stage_1_train, lgd_targets_stage_1_train)\n",
    "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
    "# with inputs (independent variables) contained in the first dataframe\n",
    "# and targets (dependent variables) contained in the second dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = lgd_inputs_stage_1_train.columns.values\n",
    "# Stores the names of the columns of a dataframe in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
    "summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n",
    "# Creates a new column in the dataframe, called 'Coefficients',\n",
    "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
    "summary_table.index = summary_table.index + 1\n",
    "# Increases the index of every row of the dataframe with 1.\n",
    "summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n",
    "# Assigns values of the row with index 0 of the dataframe.\n",
    "summary_table = summary_table.sort_index()\n",
    "# Sorts the dataframe by index.\n",
    "p_values = reg_lgd_st_1.p_values\n",
    "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
    "p_values = np.append(np.nan,np.array(p_values))\n",
    "# We add the value 'NaN' in the beginning of the variable with p-values.\n",
    "summary_table['p_values'] = p_values\n",
    "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n",
    "summary_table.index = summary_table.index + 1\n",
    "summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n",
    "summary_table = summary_table.sort_index()\n",
    "p_values = reg_lgd_st_1.p_values\n",
    "p_values = np.append(np.nan,np.array(p_values))\n",
    "summary_table['p_values'] = p_values\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_inputs_stage_1_test = lgd_inputs_stage_1_test[features_all]\n",
    "# Here we keep only the variables we need for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_inputs_stage_1_test = lgd_inputs_stage_1_test.drop(features_reference_cat, axis = 1)\n",
    "# Here we remove the dummy variable reference categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_lgd_stage_1 = reg_lgd_st_1.model.predict(lgd_inputs_stage_1_test)\n",
    "# Calculates the predicted values for the dependent variable (targets)\n",
    "# based on the values of the independent variables (inputs) supplied as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_lgd_stage_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba_lgd_stage_1 = reg_lgd_st_1.model.predict_proba(lgd_inputs_stage_1_test)\n",
    "# Calculates the predicted probability values for the dependent variable (targets)\n",
    "# based on the values of the independent variables (inputs) supplied as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba_lgd_stage_1\n",
    "# This is an array of arrays of predicted class probabilities for all classes.\n",
    "# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n",
    "# and the second value is the probability for the observation to belong to the first class, i.e. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba_lgd_stage_1 = y_hat_test_proba_lgd_stage_1[: ][: , 1]\n",
    "# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n",
    "# that is, the second element.\n",
    "# In other words, we take only the probabilities for being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba_lgd_stage_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_targets_stage_1_test_temp = lgd_targets_stage_1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_targets_stage_1_test_temp.reset_index(drop = True, inplace = True)\n",
    "# We reset the index of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs = pd.concat([lgd_targets_stage_1_test_temp, pd.DataFrame(y_hat_test_proba_lgd_stage_1)], axis = 1)\n",
    "# Concatenates two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.columns = ['lgd_targets_stage_1_test', 'y_hat_test_proba_lgd_stage_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.index = lgd_inputs_stage_1_test.index\n",
    "# Makes the index of one dataframe equal to the index of another dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
